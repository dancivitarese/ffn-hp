{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import skimage.graph as graph\n",
    "import tensorflow as tf\n",
    "from rockml.data.adapter.seismic.segy import PostStackDatum\n",
    "from rockml.data.adapter.seismic.segy.poststack import PostStackAdapter2D\n",
    "from rockml.data.pipeline import Pipeline\n",
    "from rockml.data.transformations import Composer\n",
    "from rockml.data.transformations.seismic.image import Crop2D, ScaleIntensity\n",
    "from seisfast.io.horizon import Writer\n",
    "\n",
    "import utils\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_seed(label_img: tf.Tensor,\n",
    "                   canvas: tf.Tensor,\n",
    "                   seed_class: int) -> Tuple[int, tf.Variable]:\n",
    "    input_seed = 0\n",
    "    for i in range(label_img.shape[0]):\n",
    "        el = label_img[i, 8]\n",
    "        if el == seed_class:\n",
    "            input_seed = i\n",
    "            break\n",
    "    canvas_tile = tf.Variable(canvas)\n",
    "    canvas_tile[input_seed, 0].assign(1)\n",
    "    return input_seed, canvas_tile\n",
    "\n",
    "\n",
    "def get_prediction_tiles(predictions: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(predictions, axis=3)\n",
    "\n",
    "\n",
    "def run_model(canvas: tf.Tensor,\n",
    "              model: tf.keras.Model,\n",
    "              inline_datum: Tuple[tf.Tensor, tf.Tensor],\n",
    "              break_tiles_info) -> np.ndarray:\n",
    "    seed = break_tiles_info[0]\n",
    "    size = break_tiles_info[1]\n",
    "    stride = break_tiles_info[2]\n",
    "\n",
    "    feat, label = inline_datum\n",
    "    feat = tf.reshape(feat, [feat.shape[0], feat.shape[1]])\n",
    "\n",
    "    canvas_tile = tf.Variable(canvas)\n",
    "\n",
    "    v_seed = seed\n",
    "\n",
    "    old_v_center = seed\n",
    "\n",
    "    for h_seed in range(0, (feat.shape[1] - size), stride):\n",
    "        tl_seed = (v_seed, h_seed)\n",
    "\n",
    "        feat_window = feat[tl_seed[0]: tl_seed[0] + size, tl_seed[1]: tl_seed[1] + size]\n",
    "        canvas_window = canvas_tile[tl_seed[0]: tl_seed[0] + size, tl_seed[1]: tl_seed[1] + size]\n",
    "\n",
    "        if feat_window.shape != (size, size):\n",
    "            break\n",
    "\n",
    "        mini_feat_reshape = tf.reshape(feat_window, [feat_window.shape[0], feat_window.shape[1], 1])\n",
    "        canvas_window_reshape = tf.reshape(canvas_window, [feat_window.shape[0], feat_window.shape[1], 1])\n",
    "        model_input = tf.stack([mini_feat_reshape, canvas_window_reshape], axis=2)\n",
    "        model_input = tf.reshape(model_input, [1, feat_window.shape[0], feat_window.shape[1], 2])\n",
    "\n",
    "        prediction = model(model_input)\n",
    "        output_tile = tf.reshape(get_prediction_tiles(prediction.numpy()), [size, size])\n",
    "\n",
    "        output_tile = tf.dtypes.cast(output_tile, tf.uint8, name=None)\n",
    "        # temp = (output_tile - 1) * np.random.random_integers(1, size, size ** 2).reshape((size, size))\n",
    "        # path, _ = graph.shortest_path(temp, reach=1, axis=-1, output_indexlist=True)\n",
    "        # zeros = np.zeros((size, size), dtype=np.uint8)\n",
    "        # for t in path:\n",
    "        #     zeros[t] = 1\n",
    "\n",
    "        canvas_tile[tl_seed[0]: tl_seed[0] + size, tl_seed[1]: tl_seed[1] + size].assign(output_tile)\n",
    "\n",
    "        next_v_center = np.argmax(canvas_tile[:, h_seed + int(size / 2) + stride] > 0) + 1\n",
    "        if np.abs(old_v_center - next_v_center) <= size:\n",
    "            v_seed = next_v_center - int(size / 2)\n",
    "            old_v_center = next_v_center\n",
    "\n",
    "        if v_seed < 0:\n",
    "            v_seed = 0\n",
    "        elif v_seed > feat.shape[0] + size:\n",
    "            v_seed = feat.shape[0] - size\n",
    "\n",
    "    return canvas_tile.numpy()\n",
    "\n",
    "\n",
    "def export_horizons(segy: PostStackAdapter2D,\n",
    "                    horizons: np.array,\n",
    "                    path: str) -> None:\n",
    "    for cls in horizons.keys():\n",
    "        writer = Writer(os.path.join(path, f'{cls}.xyz'))\n",
    "        writer.write('inlines', np.asarray(horizons[cls], dtype=np.float32), segy.segy_raw_data)\n",
    "\n",
    "\n",
    "def get_horizons_from_max(line_number: int,\n",
    "                          amplitudes: np.array,\n",
    "                          horizons: np.array,\n",
    "                          params: dict) -> dict:\n",
    "    crop_left = params['dataset_info']['crop'][0],\n",
    "    crop_top = params['dataset_info']['crop'][2]\n",
    "    horizon_dict = {\n",
    "        os.path.basename(l).split('.')[0]: [None] * horizons.shape[2] for l in\n",
    "        params['dataset_info']['horizons_path_list']\n",
    "    }\n",
    "\n",
    "    bad_keys = []\n",
    "\n",
    "    for idx, hrz_key in enumerate(horizon_dict.keys()):\n",
    "        if np.sum(horizons[idx]) == 0:\n",
    "            bad_keys.append(hrz_key)\n",
    "            continue\n",
    "\n",
    "        columns, rows = np.where(np.transpose(horizons[idx]))\n",
    "\n",
    "        amp = 0\n",
    "        for row, column in enumerate(columns):\n",
    "            if horizon_dict[hrz_key][column] is None:\n",
    "                horizon_dict[hrz_key][column] = [line_number, column + crop_left, rows[row] + crop_top]\n",
    "            elif amplitudes[rows[row], column] > amp:\n",
    "                horizon_dict[hrz_key][column] = [line_number, column + crop_left, rows[row] + crop_top]\n",
    "                amp = amplitudes[rows[row], column]\n",
    "\n",
    "        # Clean list up from None values (discontinuities)\n",
    "        horizon_dict[hrz_key] = [rec for rec in horizon_dict[hrz_key] if rec]\n",
    "\n",
    "    for bad_key in bad_keys:\n",
    "        del horizon_dict[bad_key]\n",
    "\n",
    "    return horizon_dict\n",
    "\n",
    "\n",
    "def predict_line(model: tf.keras.Model,\n",
    "                 params: dict,\n",
    "                 seismic_datum: PostStackDatum) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    size = params['dataset_info']['tile_shape'][0]\n",
    "    stride = params['dataset_info']['stride_shape'][0]\n",
    "\n",
    "    feat = tf.dtypes.cast(seismic_datum.features, tf.uint8, name=None)\n",
    "    label_img = tf.dtypes.cast(seismic_datum.label, tf.uint8, name=None)\n",
    "    inline_datum = (feat, label_img)\n",
    "\n",
    "    horizon_list = [None] * len(params['dataset_info']['horizons_path_list'])\n",
    "\n",
    "    for idx, horizon in enumerate(params['dataset_info']['horizons_path_list']):\n",
    "        canvas = tf.zeros(label_img.shape, tf.uint8)\n",
    "        input_seed, canvas = get_input_seed(label_img, canvas, idx)\n",
    "        canvas = utils.clean_label(canvas)\n",
    "\n",
    "        seed = input_seed - int(size / 2)\n",
    "        break_tiles_info = (seed, size, stride)\n",
    "\n",
    "        if input_seed > 0:\n",
    "            output_horizon = run_model(canvas, model, inline_datum, break_tiles_info)\n",
    "            zeros = get_graph(output_horizon)\n",
    "            # from PIL import Image\n",
    "            # Image.fromarray(output_horizon*128).save('/home/sallesd/out.png')\n",
    "            # Image.fromarray(zeros*128).save('/home/sallesd/zeros.png')\n",
    "            # Image.fromarray((zeros+output_horizon)*120).save('/home/sallesd/prod.png')\n",
    "            # horizon_list[idx] = output_horizon.astype(np.uint8) * 175\n",
    "            horizon_list[idx] = zeros * 175\n",
    "\n",
    "    raw_amplitudes = seismic_datum.features\n",
    "    clean_horizons = [i if i is not None else np.zeros([feat.shape[0], feat.shape[1]]) for i in horizon_list]\n",
    "    horizons = np.stack(clean_horizons)\n",
    "\n",
    "    return raw_amplitudes, horizons\n",
    "\n",
    "\n",
    "def get_graph(output_horizon):\n",
    "    temp = (output_horizon - 1) * np.random.random_integers(\n",
    "        1,\n",
    "        output_horizon.shape[0],\n",
    "        np.prod(output_horizon.shape)\n",
    "    ).reshape(output_horizon.shape)\n",
    "    path, _ = graph.shortest_path(temp, reach=1, axis=-1, output_indexlist=True)\n",
    "    zeros = np.zeros(output_horizon.shape, dtype=np.uint8)\n",
    "    for t in path:\n",
    "        zeros[t] = 1\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['model_path'] = '/home/sallesd/ffn_model/best.h5'\n",
    "params['output_dir'] = '/home/sallesd/ffn_model/hrz'\n",
    "params['dataset_info'] = json.load(open('/home/sallesd/ffn_dataset/info.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sallesd/miniconda/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/core.py:901: UserWarning: models.model_zoo is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "segy = PostStackAdapter2D(\n",
    "    segy_path=params['dataset_info']['segy_info']['segy_path'],\n",
    "    horizons_path_list=params['dataset_info']['horizons_path_list'],\n",
    "    data_dict={'inline': [[150, 250]]}\n",
    ")\n",
    "scan_result = segy.initial_scan()\n",
    "first_inline, last_inline = scan_result['range_inlines']\n",
    "\n",
    "pre_proc = [\n",
    "    Crop2D(\n",
    "        crop_left=params['dataset_info']['crop'][0],\n",
    "        crop_right=params['dataset_info']['crop'][1],\n",
    "        crop_top=params['dataset_info']['crop'][2],\n",
    "        crop_bottom=params['dataset_info']['crop'][3]\n",
    "    ),\n",
    "    ScaleIntensity(\n",
    "        gray_levels=params['dataset_info']['gray_levels'],\n",
    "        percentile=params['dataset_info']['percentile']\n",
    "    ),\n",
    "]\n",
    "\n",
    "composer = Composer(transformations=pre_proc)\n",
    "dataset = Pipeline(composer=composer).build_dataset(segy, 80, 80)\n",
    "\n",
    "print(composer)\n",
    "print(f'Number of lines: {len(dataset)}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing inline: 150\n",
      "Processing inline: 151\n",
      "Processing inline: 152\n",
      "Processing inline: 153\n",
      "Processing inline: 154\n"
     ]
    }
   ],
   "source": [
    "utils.makedir(params['output_dir'])\n",
    "model = tf.keras.models.load_model(params['model_path'])\n",
    "\n",
    "for datum in dataset:\n",
    "    print(f\"Processing inline: {datum.line_number}\")\n",
    "\n",
    "    # Run prediction through number of horizons\n",
    "    amplitudes, horizons = predict_line(\n",
    "        model,\n",
    "        params,\n",
    "        datum,\n",
    "    )\n",
    "\n",
    "    # Write .xyz files\n",
    "    horizon_list = get_horizons_from_max(\n",
    "        datum.line_number,\n",
    "        amplitudes,\n",
    "        horizons,\n",
    "        params\n",
    "    )\n",
    "    export_horizons(segy, horizon_list, params['output_dir'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}